{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c15d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "import csv\n",
    "import requests as rq\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c594f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_generation(object):\n",
    "    \"\"\" этот класс содержит методы для создания новых признаков\n",
    "        и сопутствующие им методы \"\"\"\n",
    "    \n",
    "    def __init__(self, df_movies):\n",
    "        \"\"\" конструктор класса \"\"\"\n",
    "        self.movies = df_movies    #исходный массив фильмов\n",
    "        self.genres = []           #список жанров\n",
    "        self.tags_corpus = []      #\"мешок\"/\"corpus\" пользовательских тегов\n",
    "        self.keywords_corpus = []  #\"мешок\"/\"corpus\" ключевых слов\n",
    "        self.overview_corpus = []  #\"мешок\"/\"corpus\" overview\n",
    "        self.actors_corpus = []    #\"мешок\"/\"corpus\" actors\n",
    "        self.crew_corpus = []      #\"мешок\"/\"corpus\" crew\n",
    "    pass\n",
    "    \n",
    "    def year_extraction(self):\n",
    "        \"\"\" извлекает год выпуска фильма и добавляет его в массив,\n",
    "            возвращает массив \"\"\"\n",
    "        def year_extract(column):\n",
    "            try:\n",
    "                year = int(column.replace(' ','')[-5:-1])\n",
    "            except ValueError:\n",
    "                year = 0\n",
    "            pass\n",
    "            return year\n",
    "        \n",
    "        self.movies['year'] = self.movies['title'].apply(year_extract)\n",
    "        return self.movies\n",
    "    pass\n",
    "    \n",
    "    def avg_users_rating(self, df_ratings):\n",
    "        \"\"\" считает средний пользовательский рейтинг фильма и добавляет его в массив,\n",
    "            возвращает массив \"\"\"\n",
    "        def avg_rating(column):\n",
    "            avg_rating = df_ratings[df_ratings['movieId'] == column]['rating'].mean()\n",
    "            if pd.isna(avg_rating):\n",
    "                return 0\n",
    "            else: \n",
    "                return avg_rating\n",
    "            pass\n",
    "    \n",
    "        self.movies['avg_rating'] = self.movies['movieId'].apply(avg_rating)\n",
    "        return self.movies\n",
    "    pass\n",
    "    \n",
    "    def rebuild_genre_list(self):\n",
    "        \"\"\" получает список жанров фильмов, сохраняет в переменную класса \"genres\",\n",
    "            возвращает список жанров \"\"\"\n",
    "        def genre_list(column):\n",
    "            genres_split = column.split('|')\n",
    "            if not self.genres:\n",
    "                self.genres.append(genres_split[0])\n",
    "            pass\n",
    "            for genre in genres_split:\n",
    "                if genre not in self.genres:\n",
    "                    self.genres.append(genre)\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        _ = self.movies['genres'].apply(genre_list)\n",
    "        self.genres.sort()\n",
    "        return self.genres\n",
    "    pass\n",
    "        \n",
    "    def genre_vectorization(self, rebuild_genre_list = False):\n",
    "        \"\"\" для каждого фильма получает вектор жанров и добавляет его в массив,\n",
    "            возвращает массив \"\"\"\n",
    "        #если список жанров пуст или нужно его перестроить\n",
    "        if not self.genres or rebuild_genre_list:\n",
    "            _ = self.rebuild_genre_list()\n",
    "        pass\n",
    "        #получаем векторы жанров для каждого фильма\n",
    "        def genre_vect(column):\n",
    "            genre_vect = []\n",
    "            genres_split = column.split('|')\n",
    "            for genre in self.genres:\n",
    "                if genre in genres_split:\n",
    "                    genre_vect.append(1) #'1' в позиции, если фильм отмечен жанром\n",
    "                else:\n",
    "                    genre_vect.append(0) #'0' в позиции, если фильм НЕ отмечен жанром\n",
    "                pass\n",
    "            pass\n",
    "            return genre_vect\n",
    "    \n",
    "        self.movies['genres_vect'] = self.movies['genres'].apply(genre_vect)\n",
    "        return self.movies\n",
    "    pass\n",
    " \n",
    "    def tags_vectorization(self, df_tags):\n",
    "        \"\"\" для каждого фильма получает вектор пользовательских тегов и добавляет его в массив,\n",
    "            возвращает массив \"\"\"\n",
    "        \n",
    "        def tag_prep(message, stop_words, morph):\n",
    "            if str(message) == 'nan':\n",
    "                message = ''\n",
    "            message_ = re.sub(r\"[^A-Za-z ]\",\" \", message)                       #оставляем только буквы\n",
    "            tokens = word_tokenize(message_)                                    #разбиваем на слова\n",
    "            tokens = [i for i in tokens if (i not in stop_words)]               #исключаем стопслова\n",
    "            tokens = list(map(lambda x: morph.parse(x)[0].normal_form, tokens)) #приводим к нормальной форме\n",
    "            return tokens\n",
    "    \n",
    "        morph = pymorphy2.MorphAnalyzer() #класс для получения нормальной формы (инфинитива) слова\n",
    "        stop_words = stopwords.words('english') #подключаем стопслова\n",
    "\n",
    "        #получаем список токенов\n",
    "        tokens_list = df_tags['tag'].apply(tag_prep, stop_words = stop_words, morph = morph)\n",
    "\n",
    "        #собираем \"мешок слов\"/\"корпус\" из токенов (текстов) сообщений\n",
    "        self.tags_corpus = [\" \".join(tokens) for tokens in tokens_list]\n",
    "\n",
    "        #получаем массив векторов пользовательских тегов\n",
    "        vectors = CountVectorizer().fit_transform(self.tags_corpus).toarray()\n",
    "\n",
    "        #зафиксируем суммарный вектор тегов фильма\n",
    "        def vect_summ(column):   \n",
    "            vect_summ = np.zeros(vectors.shape[1], dtype = int)       #нулевой вектор\n",
    "            for index in df_tags[df_tags['movieId'] == column].index: #суммируем векторы тегов фильма\n",
    "                vect_summ = vect_summ | vectors[index]\n",
    "            pass\n",
    "            return vect_summ\n",
    "\n",
    "        self.movies['tags_vect'] = self.movies['movieId'].apply(vect_summ)\n",
    "        return self.movies\n",
    "    pass\n",
    "        \n",
    "    def req_GetKeywords(self, api_key, df_links):\n",
    "        \"\"\" для каждого фильма получает ключевые слова, сохраняет в файл, создает вектор ключевых слов,\n",
    "            добавляет его в массив, возвращает массив \"\"\"\n",
    "            \n",
    "        with open('req_GetKeywords.csv', \"w\", newline='\\n') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(['movieId', 'keywords'])\n",
    "            \n",
    "        for item in df_links.index:\n",
    "            movieId = df_links.at[item, 'movieId']\n",
    "            if not np.isnan(df_links.at[item, 'tmdbId']):\n",
    "                tmdbId = int(df_links.at[item, 'tmdbId'])\n",
    "                #получаем инфу по очередному фильму\n",
    "                res = rq.get(f'https://api.themoviedb.org/3/movie/{tmdbId}/keywords?api_key={api_key}')\n",
    "                if res.status_code == 200: #если есть ответ\n",
    "                    keywords = pd.DataFrame.from_dict(res.json()['keywords'])\n",
    "                    if len(keywords) > 0:\n",
    "                        keywords_str = \"|\".join(keywords['name'])\n",
    "                    pass\n",
    "                    #записываем все в файл\n",
    "                    with open('req_GetKeywords.csv', \"a\", newline='\\n') as csv_file:\n",
    "                        writer = csv.writer(csv_file, delimiter=',')\n",
    "                        writer.writerow([movieId, keywords_str])\n",
    "                    pass\n",
    "                pass\n",
    "            #время задержки итерации для предотвращения блокировки ресурсом\n",
    "            #time.sleep(0.2)\n",
    "            #вывод прогресса\n",
    "            clear_output(wait = True)\n",
    "            print(f'Прогресс: {item + 1} / {df_links.shape[0]}.')\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "    def add_Keywords(self):\n",
    "        try:\n",
    "            #векторизация ключевых слов\n",
    "            keywords = pd.read_csv('req_GetKeywords.csv', ',')\n",
    "    \n",
    "            def keywords_prep(message, stop_words, morph):\n",
    "                if str(message) == 'nan':\n",
    "                    message = ''\n",
    "                message_ = message.split('|')\n",
    "                message_ = re.sub(r\"[^A-Za-z ]\",\" \", message)                       #оставляем только буквы\n",
    "                tokens = word_tokenize(message_)                                    #разбиваем на слова\n",
    "                tokens = [i for i in tokens if (i not in stop_words)]               #исключаем стопслова\n",
    "                tokens = list(map(lambda x: morph.parse(x)[0].normal_form, tokens)) #приводим к нормальной форме\n",
    "                return tokens\n",
    "            pass\n",
    "    \n",
    "            morph = pymorphy2.MorphAnalyzer() #класс для получения нормальной формы (инфинитива) слова\n",
    "            stop_words = stopwords.words('english') #подключаем стопслова\n",
    "\n",
    "            #получаем список токенов\n",
    "            keywords['tokens'] = keywords['keywords'].apply(keywords_prep, stop_words = stop_words, morph = morph)\n",
    "\n",
    "            #собираем \"мешок слов\"/\"корпус\" из токенов (текстов) сообщений\n",
    "            self.keywords_corpus = [\" \".join(tokens) for tokens in keywords['tokens']]\n",
    "\n",
    "            #получаем массив векторов пользовательских тегов\n",
    "            vectors = CountVectorizer().fit_transform(self.keywords_corpus).toarray()\n",
    "            keywords['keywords_vect'] = [vectors[i] for i in range(0, len(vectors))]\n",
    "    \n",
    "            #фиксируем в массиве фильмов вектор ключевых слов фильма, если есть ключевые слова, \n",
    "            #либо нулевой вектор\n",
    "            def keywords_vect(column):   \n",
    "                vect_summ = np.zeros(vectors.shape[1], dtype = int)       #нулевой вектор\n",
    "                for index in keywords[keywords['movieId'] == column].index: #суммируем векторы тегов фильма\n",
    "                    vect_summ = vect_summ | vectors[index]\n",
    "                pass\n",
    "                return vect_summ\n",
    "            pass\n",
    "\n",
    "            self.movies['keywords_vect'] = self.movies['movieId'].apply(keywords_vect)\n",
    "            return self.movies\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print('Не найден файл: req_GetKeywords.csv')\n",
    "    pass\n",
    "\n",
    "    def req_GetOverview(self, api_key, df_links):\n",
    "        \"\"\" для каждого фильма получает overview, сохраняет в файл, создает вектор текста overview,\n",
    "            добавляет его в массив, возвращает массив \"\"\"\n",
    "            \n",
    "        with open('req_GetOverview.csv', \"w\", newline='\\n') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(['movieId', 'overview'])\n",
    "            \n",
    "        for item in df_links.index:\n",
    "            movieId = df_links.at[item, 'movieId']\n",
    "            if not np.isnan(df_links.at[item, 'tmdbId']):\n",
    "                tmdbId = int(df_links.at[item, 'tmdbId'])\n",
    "                #получаем инфу по очередному фильму\n",
    "                res = rq.get(f'https://api.themoviedb.org/3/movie/{tmdbId}?api_key={api_key}&language=en-US')\n",
    "                if res.status_code == 200: #если есть ответ, записываем все в файл\n",
    "                    with open('req_GetOverview.csv', \"a\", newline='\\n') as csv_file:\n",
    "                        writer = csv.writer(csv_file, delimiter=',')\n",
    "                        writer.writerow([movieId, res.json()['overview']])\n",
    "                    pass\n",
    "                pass\n",
    "            #время задержки итерации для предотвращения блокировки ресурсом\n",
    "            #time.sleep(0.2)\n",
    "            #вывод прогресса\n",
    "            clear_output(wait = True)\n",
    "            print(f'Прогресс: {item + 1} / {df_links.shape[0]}.')\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "    def add_Overviews(self):\n",
    "        try:\n",
    "            #векторизация ключевых слов\n",
    "            overview = pd.read_csv('req_GetOverview.csv', ',')\n",
    "    \n",
    "            def keywords_prep(message, stop_words, morph):\n",
    "                if str(message) == 'nan':\n",
    "                    message = ''\n",
    "                message_ = re.sub(r\"[^A-Za-z ]\",\" \", message)                       #оставляем только буквы\n",
    "                tokens = word_tokenize(message_)                                    #разбиваем на слова\n",
    "                tokens = [i for i in tokens if (i not in stop_words)]               #исключаем стопслова\n",
    "                tokens = list(map(lambda x: morph.parse(x)[0].normal_form, tokens)) #приводим к нормальной форме\n",
    "                return tokens\n",
    "            pass\n",
    "    \n",
    "            morph = pymorphy2.MorphAnalyzer() #класс для получения нормальной формы (инфинитива) слова\n",
    "            stop_words = stopwords.words('english') #подключаем стопслова\n",
    "\n",
    "            #получаем список токенов\n",
    "            overview['tokens'] = overview['overview'].apply(keywords_prep, stop_words = stop_words, morph = morph)\n",
    "\n",
    "            #собираем \"мешок слов\"/\"корпус\" из токенов (текстов) сообщений\n",
    "            self.overview_corpus = [\" \".join(tokens) for tokens in overview['tokens']]\n",
    "\n",
    "            #получаем массив векторов пользовательских тегов\n",
    "            vectors = CountVectorizer().fit_transform(self.overview_corpus).toarray()\n",
    "            overview['overview_vect'] = [vectors[i] for i in range(0, len(vectors))]\n",
    "    \n",
    "            #фиксируем в массиве фильмов вектор ключевых слов фильма, если есть ключевые слова, \n",
    "            #либо нулевой вектор\n",
    "            def overview_vect(column):   \n",
    "                vect_summ = np.zeros(vectors.shape[1], dtype = int)       #нулевой вектор\n",
    "                for index in overview[overview['movieId'] == column].index: #суммируем векторы тегов фильма\n",
    "                    vect_summ = vect_summ | vectors[index]\n",
    "                pass\n",
    "                return vect_summ\n",
    "            pass\n",
    "\n",
    "            self.movies['overview_vect'] = self.movies['movieId'].apply(overview_vect)\n",
    "            return self.movies\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print('Не найден файл: req_GetOverview.csv')\n",
    "    pass\n",
    "    \n",
    "    def req_GetPlayLists(self, api_key, df_links):\n",
    "        \"\"\" для каждого фильма получает количество пользовательских play_lists, сохраняет в файл, \n",
    "            возвращает массив \"\"\"\n",
    "     \n",
    "        with open('req_GetPlayLists.csv', \"w\", newline='\\n') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(['movieId', 'playlists'])\n",
    "            \n",
    "        for item in df_links.index:\n",
    "            movieId = df_links.at[item, 'movieId']\n",
    "            if not np.isnan(df_links.at[item, 'tmdbId']):\n",
    "                tmdbId = int(df_links.at[item, 'tmdbId'])\n",
    "                #получаем инфу по очередному фильму\n",
    "                res = rq.get(f'https://api.themoviedb.org/3/movie/{tmdbId}/lists?api_key={api_key}&language=en-US&page=1')\n",
    "                if res.status_code == 200: #если есть ответ, записываем все в файл\n",
    "                    with open('req_GetPlayLists.csv', \"a\", newline='\\n') as csv_file:\n",
    "                        writer = csv.writer(csv_file, delimiter=',')\n",
    "                        writer.writerow([movieId, res.json()['total_results']])\n",
    "                pass\n",
    "            pass\n",
    "            \n",
    "            #time.sleep(0.2) #время задержки итерации для предотвращения блокировки ресурсом\n",
    "            clear_output(wait = True) #вывод прогресса\n",
    "            print(f'Прогресс: {item + 1} / {df_links.shape[0]}.')\n",
    "        pass\n",
    "    pass\n",
    "      \n",
    "    def add_Playlists(self):\n",
    "        try:\n",
    "            play_lists = pd.read_csv('req_GetPlayLists.csv', ',')\n",
    "       \n",
    "            def playlists_set(column):   \n",
    "                total_playlists = 0\n",
    "                for index in play_lists[play_lists['movieId'] == column].index: #суммируем playlists фильма\n",
    "                    total_playlists += play_lists.at[index, 'playlists']\n",
    "                pass\n",
    "                return total_playlists\n",
    "            pass\n",
    "        \n",
    "            self.movies['playlists'] = self.movies['movieId'].apply(playlists_set)\n",
    "            return self.movies\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print('Не найден файл: req_GetPlayLists.csv')\n",
    "    pass\n",
    "        \n",
    "    def req_GetDetails(self, api_key, df_links):\n",
    "        \"\"\" для каждого фильма получает некоторые его характеристики, сохраняет в файл, \n",
    "            возвращает массив \"\"\"\n",
    "            \n",
    "        new_columns = ['adult', 'budget', 'popularity', 'revenue', 'vote_average', 'vote_count']\n",
    "\n",
    "        with open('req_GetDetails.csv', \"w\", newline='\\n') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(new_columns)\n",
    "            \n",
    "        for item in df_links.index:\n",
    "            movieId = df_links.at[item, 'movieId']\n",
    "            if not np.isnan(df_links.at[item, 'tmdbId']):\n",
    "                tmdbId = int(df_links.at[item, 'tmdbId'])\n",
    "                #получаем инфу по очередному фильму\n",
    "                res = rq.get(f'https://api.themoviedb.org/3/movie/{tmdbId}?api_key={api_key}&language=en-US')\n",
    "                if res.status_code == 200: #если есть ответ, записываем все в файл\n",
    "                    with open('req_GetDetails.csv', \"a\", newline='\\n') as csv_file:\n",
    "                        writer = csv.writer(csv_file, delimiter=',')\n",
    "                        writer.writerow([movieId, \\\n",
    "                                         res.json()['adult'], \\\n",
    "                                         res.json()['budget'], \\\n",
    "                                         res.json()['popularity'], \\\n",
    "                                         res.json()['revenue'], \\\n",
    "                                         res.json()['vote_average'], \\\n",
    "                                         res.json()['vote_count']])\n",
    "                    pass\n",
    "                pass\n",
    "            pass\n",
    "            \n",
    "            #time.sleep(0.2) #время задержки итерации для предотвращения блокировки ресурсом\n",
    "            clear_output(wait = True) #вывод прогресса\n",
    "            print(f'Прогресс: {item + 1} / {df_links.shape[0]}.')\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "    def add_Details(self):\n",
    "        try:\n",
    "            new_columns = ['adult', 'budget', 'popularity', 'revenue', 'vote_average', 'vote_count']\n",
    "            default_values = [False, 0.0, 0.0, 0.0, 0.0, 0]\n",
    "            \n",
    "            #заполняем массив movies новыми признаками\n",
    "            details = pd.read_csv('req_GetDetails.csv', ',')\n",
    "        \n",
    "            def details_set(row):\n",
    "                if len(details[details['movieId'] == row['movieId']].index) > 0:\n",
    "                    index = details[details['movieId'] == row['movieId']].index[0]\n",
    "                    for column in new_columns:\n",
    "                        row[column] = details.at[index, column]\n",
    "                    pass\n",
    "                else:\n",
    "                    for ind, column in enumerate(new_columns):\n",
    "                        row[column] = default_values[ind]\n",
    "                    pass\n",
    "                pass\n",
    "                return row\n",
    "            pass\n",
    "        \n",
    "            self.movies = self.movies.apply(details_set, axis = 1)\n",
    "            return self.movies\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print('Не найден файл: req_GetDetails.csv')\n",
    "    pass\n",
    "     \n",
    "    def req_GetCredits(self, api_key, df_links):\n",
    "        \"\"\" для каждого фильма получает список актеров и съемочной группы и их рейтинг, \n",
    "            сохраняет в файл, получает векторы списков и добавляет их в массив, возвращает массив \"\"\"\n",
    "            \n",
    "        with open('req_GetCredits.csv', \"w\", newline='\\n') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(['movieId', 'actors', 'sum_act_pop', 'crew', 'sum_crew_pop'])\n",
    "            \n",
    "        for item in df_links.index: #[8613 : df_links.index.max() + 1]:\n",
    "            movieId = df_links.at[item, 'movieId']\n",
    "            if not np.isnan(df_links.at[item, 'tmdbId']):\n",
    "                tmdbId = int(df_links.at[item, 'tmdbId'])\n",
    "                #получаем инфу по очередному фильму\n",
    "                res = rq.get(f'https://api.themoviedb.org/3/movie/{tmdbId}/credits?api_key={api_key}&language=en-US')\n",
    "                if res.status_code == 200: #если есть ответ\n",
    "                    #актеры\n",
    "                    cast = pd.DataFrame.from_dict(res.json()['cast'])\n",
    "                    if len(cast) > 0:\n",
    "                        sum_act_pop = cast['popularity'].sum()\n",
    "                        cast_sort = cast.sort_values(by = 'popularity', ascending = False)\n",
    "                        actors = \"|\".join(cast_sort['original_name']) #[: min(5, len(cast_sort))])\n",
    "                    else:\n",
    "                        sum_act_pop = 0.0\n",
    "                        actors = 'none'\n",
    "                    pass\n",
    "                    crew = pd.DataFrame.from_dict(res.json()['crew'])\n",
    "                    if len(crew) > 0:\n",
    "                        sum_crew_pop = crew['popularity'].sum()\n",
    "                        crew_sort = crew.sort_values(by = 'popularity', ascending = False)\n",
    "                        crews = \"|\".join(crew_sort['original_name']) #[: min(5, len(crew_sort))])\n",
    "                    else:\n",
    "                        sum_crew_pop = 0.0\n",
    "                        crews = 'none'\n",
    "                    pass\n",
    "                    #записываем все в файл\n",
    "                    with open('req_GetCredits.csv', \"a\", newline='\\n') as csv_file:\n",
    "                        writer = csv.writer(csv_file, delimiter=',')\n",
    "                        writer.writerow([movieId, \\\n",
    "                                         actors, \\\n",
    "                                         sum_act_pop, \\\n",
    "                                         crews, \\\n",
    "                                         sum_crew_pop])\n",
    "                    pass\n",
    "                pass\n",
    "            pass\n",
    "            \n",
    "            #time.sleep(0.2) #время задержки итерации для предотвращения блокировки ресурсом\n",
    "            \n",
    "            clear_output(wait = True) #вывод прогресса\n",
    "            print(f'Прогресс: {item + 1} / {df_links.shape[0]}.')\n",
    "        pass\n",
    "    pass\n",
    "     \n",
    "    def add_Credits(self):\n",
    "        try:\n",
    "            #заполняем массив movies новыми признаками\n",
    "            credits = pd.read_csv('req_GetCredits.csv', ',')\n",
    "        \n",
    "            #векторизация списка актеров и списка съемочной группы\n",
    "            def fio_prep(actors):\n",
    "                tokens = []\n",
    "                fios = actors.split('|') #разбиваем ФИО\n",
    "                for fio in fios:\n",
    "                    token = word_tokenize(fio) #разбиваем на слова\n",
    "                    tokens.extend(token)   \n",
    "                return tokens #возвращаем токен\n",
    "            pass\n",
    "         \n",
    "            #получаем список токенов\n",
    "            credits['act_tokens'] = credits['actors'].apply(fio_prep)\n",
    "            credits['crew_tokens'] = credits['crew'].apply(fio_prep)\n",
    "\n",
    "            #собираем \"мешок слов\"/\"корпус\" из токенов (текстов) сообщений\n",
    "            self.actors_corpus = [\" \".join(tokens) for tokens in credits['act_tokens']]\n",
    "            self.crew_corpus = [\" \".join(tokens) for tokens in credits['crew_tokens']]\n",
    "\n",
    "            #получаем массив векторов списка актеров и векторов списка съемочной группы\n",
    "            act_vectors = CountVectorizer().fit_transform(self.actors_corpus).toarray()\n",
    "            crew_vectors = CountVectorizer().fit_transform(self.crew_corpus).toarray()\n",
    "    \n",
    "            #фиксируем в массиве фильмов вектор актеров, если есть список актеров, либо нулевой вектор\n",
    "            def act_crew_vect(row):   \n",
    "                if len(credits[credits['movieId'] == row['movieId']].index) > 0:\n",
    "                    index = credits[credits['movieId'] == row['movieId']].index[0] #если есть инф. о фильме\n",
    "                    row['act_vect'] = act_vectors[index]\n",
    "                    row['crew_vect'] = crew_vectors[index]\n",
    "                    row['sum_act_pop'] = credits.at[index, 'sum_act_pop']\n",
    "                    row['sum_crew_pop'] = credits.at[index, 'sum_crew_pop']\n",
    "                else:\n",
    "                    row['act_vect'] = np.zeros(act_vectors.shape[1], dtype = int)   #если нет инф. о фильме\n",
    "                    row['crew_vect'] = np.zeros(crew_vectors.shape[1], dtype = int)\n",
    "                    row['sum_act_pop'] = 0.0\n",
    "                    row['sum_crew_pop'] = 0.0\n",
    "                pass\n",
    "                return row\n",
    "            pass\n",
    "\n",
    "            self.movies = self.movies.apply(act_crew_vect, axis = 1)\n",
    "            return self.movies\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print('Не найден файл: req_GetCredits.csv')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa53429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#не выполняется, если запущен, как \"модуль\"\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6f903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "670343d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a9a223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a42ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
